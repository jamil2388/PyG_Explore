{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41fb50f-4ef4-4741-901b-4a62e6f611e3",
   "metadata": {},
   "source": [
    "# Continuation from GNN_Hands_on_Part_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f3a59-7c31-4119-a5f5-75ab7d8391b1",
   "metadata": {},
   "source": [
    "## Previously we explored : \n",
    "\n",
    "1. Introduction to Graph Data Structure in PyG\n",
    "2. Homogeneous Data Example\n",
    "3. Loading a Default Dataset\n",
    "4. Creating Custom Graph Data\n",
    "5. Splitting the Data\n",
    "6. Working with Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850dbd2-eda4-4c2e-9dca-6e32f2bf8ff1",
   "metadata": {},
   "source": [
    "#### Cora Dataset\n",
    "\n",
    "The Cora dataset is a well-known benchmark dataset used in graph neural networks, particularly for node classification tasks. It consists of scientific publications, which are represented as nodes, and citations between these publications, which are represented as edges\n",
    "\n",
    "Components of the Cora Dataset\n",
    "\n",
    "Nodes:\n",
    "- Each node represents a paper in the dataset\n",
    "- There are a total of 2,708 papers (nodes) in the Cora dataset\n",
    "\n",
    "Edges:\n",
    "- Each edge represents a citation from one paper to another\n",
    "- The dataset contains 5,429 edges (citations), indicating the relationships between the papers\n",
    "\n",
    "Node Features:\n",
    "- Each paper has a feature vector that describes its content, typically based on the words in the paper.\n",
    "- The Cora dataset has 1,433 unique features derived from the words in the papers, which are represented in a binary format (1 if a word is present, 0 if not).\n",
    "\n",
    "Labels:\n",
    "- Each node (paper) belongs to one of seven classes. These classes represent different topics or categories of research.\n",
    "- The classes are: Neural Networks, Genetic Algorithms, Probabilistic Methods, Reinforcement Learning, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb31c544-5a32-4fd2-b44f-69d49dd7e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]  # We only have 1 single graph in this dataset\n",
    "\n",
    "# Create a DataLoader for mini-batching\n",
    "# Here, we specify a batch size of 64\n",
    "loader = DataLoader(data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0998b8-701b-47b9-a9e6-0bde16b0808a",
   "metadata": {},
   "source": [
    "## Table of Contents for the Current Notebook\n",
    "\n",
    "7. Creating a GNN Model\n",
    "8. Defining Train and Evaluation Methods\n",
    "9. Demonstrating Node Classification Task\n",
    "10. Demonstrating Graph Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2d319-7119-41b8-b2bd-794d5d0f1a95",
   "metadata": {},
   "source": [
    "## 7. Creating a GNN Model\n",
    "\n",
    "To work with graph data, we need to define a Graph Neural Network (GNN) model. PyG provides a variety of GNN layers that can be used to build models for node classification, graph classification, and other tasks.\n",
    "\n",
    "We'll cover:\n",
    "- Choosing the appropriate GNN layer (e.g., `GCNConv`, `GATConv`, etc.)\n",
    "- Defining the GNN architecture (input, hidden, and output layers)\n",
    "- Incorporating activation functions, dropout, and other regularization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f4f019f-92e9-4b1d-841f-e2232d0adbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define the Graph Convolutional Network (GCN)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the layers of the GCN\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)  # Second GCN layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Forward pass through the first GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)  # Apply ReLU non-linearity\n",
    "        # Forward pass through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)  # Apply log softmax for multi-class classification\n",
    "\n",
    "# Example of initializing the model\n",
    "input_dim = dataset.num_features  # Number of node features (from the dataset)\n",
    "hidden_dim = 16  # Arbitrary number of hidden units\n",
    "output_dim = dataset.num_classes  # Number of classes (from the dataset)\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Display the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a271a1-90a0-4715-982a-8f6137fe400f",
   "metadata": {},
   "source": [
    "## 8. Defining Train and Evaluation Methods\n",
    "\n",
    "Once we have our model, it's important to define methods for training and evaluating its performance. In this section, we'll build the core training loop and evaluation function.\n",
    "\n",
    "We'll explore:\n",
    "- Setting up a loss function (e.g., Cross Entropy for classification)\n",
    "- Optimizing with an appropriate optimizer (e.g., Adam)\n",
    "- Tracking performance metrics (e.g., accuracy, loss)\n",
    "- How to implement the training loop with backpropagation\n",
    "- How to evaluate the model on validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee35ca-573c-4fb4-938a-9eae09817978",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d980dd-efa2-4173-835d-a8dc36b8e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Calculate the loss on the training nodes\n",
    "    loss.backward()  # Backpropagate the gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "    return loss.item()  # Return the loss value for logging\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        out = model(data.x, data.edge_index)\n",
    "        # Get predictions for the nodes in the test set\n",
    "        pred = out[data.test_mask].argmax(dim=1)\n",
    "        # Calculate accuracy by comparing predictions to true labels\n",
    "        accuracy = accuracy_score(data.y[data.test_mask].cpu(), pred.cpu())\n",
    "    return accuracy  # Return the accuracy value for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f95583-5a0d-4358-83d7-85d982bd1495",
   "metadata": {},
   "source": [
    "## 9. Demonstrating Node Classification Task\n",
    "\n",
    "Node classification is a common task in GNNs, where the goal is to predict labels for individual nodes in a graph.\n",
    "\n",
    "We'll demonstrate:\n",
    "- How to set up a node classification task using a default or custom dataset\n",
    "- Training the GNN model to classify nodes\n",
    "- Evaluating performance using accuracy or other relevant metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c073747-46d0-412b-a66e-864e6be001ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 0.0162\n",
      "Epoch 010, Loss: 0.0122\n",
      "Epoch 020, Loss: 0.0098\n",
      "Epoch 030, Loss: 0.0086\n",
      "Epoch 040, Loss: 0.0080\n",
      "Epoch 050, Loss: 0.0076\n",
      "Epoch 060, Loss: 0.0073\n",
      "Epoch 070, Loss: 0.0072\n",
      "Epoch 080, Loss: 0.0070\n",
      "Epoch 090, Loss: 0.0070\n",
      "Test Accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example: Setting up the optimizer and loss criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Example: Training loop\n",
    "for epoch in range(100):  # Train for 100 epochs\n",
    "    loss = train(model, data, optimizer, criterion)\n",
    "    if epoch % 10 == 0:  # Log every 10 epochs\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')\n",
    "        \n",
    "# Example: Evaluating the model\n",
    "accuracy = evaluate(model, data)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd5a78-cd13-4411-8d34-fd55545f3eeb",
   "metadata": {},
   "source": [
    "## 10. Demonstrating Graph Classification Task\n",
    "\n",
    "Graph classification is another important task, where the aim is to predict a label for an entire graph. We use the MUTAG Dataset\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.datasets.TUDataset.html#torch_geometric.datasets.TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ad259-6567-4b8b-a8dd-e6899e319e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load the MUTAG dataset\n",
    "dataset = TUDataset(root='data/MUTAG', name='MUTAG')\n",
    "\n",
    "# Print dataset properties\n",
    "print(f'Dataset: {dataset.name}')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188a06e-38af-499e-ac55-cc5eb9855352",
   "metadata": {},
   "source": [
    "##### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "751d4eb4-92ae-4bc1-bc9b-3a7c62b18075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes for splitting\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "val_size = int(0.15 * len(dataset))    # 15% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# Shuffle the dataset and create indices for splitting\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_data = dataset[indices[:train_size]]\n",
    "val_data = dataset[indices[train_size:train_size + val_size]]\n",
    "test_data = dataset[indices[train_size + val_size:]]\n",
    "\n",
    "# Create loaders for train, validation, and test splits\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a804de7-2286-42ed-954f-38c23862a409",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### Create the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdf6c54e-e257-42ac-8c54-12ce37788500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch.nn import Sequential, Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import global_mean_pool \n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, 64)   # Change to GCNConv to see a comparison\n",
    "        self.conv2 = SAGEConv(64, 64)                  # Change to GCNConv to see a comparison\n",
    "        self.fc = Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)  # Global pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "499aa8e8-e75f-4e06-a72a-2a51b033e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89005d6d-6e62-4073-b402-1b15a80ce9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 2/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 3/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 4/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 5/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 6/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 7/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 8/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 9/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 10/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 11/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 12/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 13/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 14/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 15/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 16/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 17/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 18/50, Train Accuracy: 0.7099, Validation Accuracy: 0.6071\n",
      "Epoch 19/50, Train Accuracy: 0.7176, Validation Accuracy: 0.6071\n",
      "Epoch 20/50, Train Accuracy: 0.7328, Validation Accuracy: 0.6429\n",
      "Epoch 21/50, Train Accuracy: 0.7786, Validation Accuracy: 0.6786\n",
      "Epoch 22/50, Train Accuracy: 0.7786, Validation Accuracy: 0.6786\n",
      "Epoch 23/50, Train Accuracy: 0.7786, Validation Accuracy: 0.6786\n",
      "Epoch 24/50, Train Accuracy: 0.7252, Validation Accuracy: 0.6429\n",
      "Epoch 25/50, Train Accuracy: 0.7710, Validation Accuracy: 0.7143\n",
      "Epoch 26/50, Train Accuracy: 0.7863, Validation Accuracy: 0.7143\n",
      "Epoch 27/50, Train Accuracy: 0.7786, Validation Accuracy: 0.7143\n",
      "Epoch 28/50, Train Accuracy: 0.7710, Validation Accuracy: 0.7143\n",
      "Epoch 29/50, Train Accuracy: 0.7557, Validation Accuracy: 0.7143\n",
      "Epoch 30/50, Train Accuracy: 0.4962, Validation Accuracy: 0.5714\n",
      "Epoch 31/50, Train Accuracy: 0.6870, Validation Accuracy: 0.7143\n",
      "Epoch 32/50, Train Accuracy: 0.7710, Validation Accuracy: 0.7143\n",
      "Epoch 33/50, Train Accuracy: 0.7710, Validation Accuracy: 0.6429\n",
      "Epoch 34/50, Train Accuracy: 0.7176, Validation Accuracy: 0.6429\n",
      "Epoch 35/50, Train Accuracy: 0.7176, Validation Accuracy: 0.6429\n",
      "Epoch 36/50, Train Accuracy: 0.7176, Validation Accuracy: 0.6429\n",
      "Epoch 37/50, Train Accuracy: 0.7481, Validation Accuracy: 0.6429\n",
      "Epoch 38/50, Train Accuracy: 0.7557, Validation Accuracy: 0.7143\n",
      "Epoch 39/50, Train Accuracy: 0.7634, Validation Accuracy: 0.7143\n",
      "Epoch 40/50, Train Accuracy: 0.7557, Validation Accuracy: 0.7143\n",
      "Epoch 41/50, Train Accuracy: 0.7634, Validation Accuracy: 0.6429\n",
      "Epoch 42/50, Train Accuracy: 0.7634, Validation Accuracy: 0.6786\n",
      "Epoch 43/50, Train Accuracy: 0.7939, Validation Accuracy: 0.7143\n",
      "Epoch 44/50, Train Accuracy: 0.7863, Validation Accuracy: 0.7143\n",
      "Epoch 45/50, Train Accuracy: 0.7634, Validation Accuracy: 0.7143\n",
      "Epoch 46/50, Train Accuracy: 0.7557, Validation Accuracy: 0.7143\n",
      "Epoch 47/50, Train Accuracy: 0.7634, Validation Accuracy: 0.7143\n",
      "Epoch 48/50, Train Accuracy: 0.7939, Validation Accuracy: 0.7143\n",
      "Epoch 49/50, Train Accuracy: 0.8015, Validation Accuracy: 0.7143\n",
      "Epoch 50/50, Train Accuracy: 0.7939, Validation Accuracy: 0.7143\n",
      "Test Accuracy: 0.6552\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = GNNModel(num_node_features=dataset.num_node_features, num_classes=dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    train_acc = evaluate(model, train_loader)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Evaluate on the test set after training\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
